{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809a94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib.request as ur\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import IFrame, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65272af",
   "metadata": {},
   "source": [
    "<h3> Creating a client object to call the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa5f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x112069890>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # or you can explicitly pass in the key (NOT RECOMMENDED)\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f27b5a",
   "metadata": {},
   "source": [
    "<h3> The GPT models: </h3>\n",
    "    \n",
    "Here is an example to call the completions API and check that the key is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d174f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can describe image content using OpenAI by leveraging its language model to generate a natural language description of the visual elements within the image. You can input the image into OpenAI's system and use the model to generate a description of the objects, scenes, and context depicted in the image. The model can provide insights and details about the image content, allowing you to effectively describe it in text form.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"how to I can describe the image content using OpenAI?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bb980",
   "metadata": {},
   "source": [
    "<h4>Getting information about the tokens consumption for this request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e82e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion_tokens': 110, 'prompt_tokens': 30, 'total_tokens': 140}\n"
     ]
    }
   ],
   "source": [
    "print(response.model_dump()['usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e183",
   "metadata": {},
   "source": [
    "You can compute the cost of each call using the `chat.compleations` API with the following formula: `((promt_tokens * <cost_of_input_tokens>) + (completion_tokens * <cost_of_output_tokens>)) / 1000`\n",
    "\n",
    "For instance, this request using  `gpt-3.5-turbo-1106` the cost is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe46586a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total cost for API call: $0.00025 USD'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_calculator_for_GPT_3_5_turbo(response):\n",
    "\n",
    "    # These 2 values are valid only for the \"gpt-3.5-turbo-1106\" model.\n",
    "    # Check https://openai.com/pricing for up-to-date prices\n",
    "    cost_of_input_tokens = 0.001\n",
    "    cost_of_output_tokens = 0.002\n",
    "\n",
    "    completion_tokens = response.model_dump()['usage']['completion_tokens']\n",
    "    prompt_tokens = response.model_dump()['usage']['prompt_tokens']\n",
    "\n",
    "    total_cost = (\n",
    "        (prompt_tokens * cost_of_input_tokens) + (completion_tokens * cost_of_output_tokens)\n",
    "    ) / 1000\n",
    "\n",
    "    return f\"Total cost for API call: ${total_cost} USD\"\n",
    "\n",
    "cost_calculator_for_GPT_3_5_turbo(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3899f9",
   "metadata": {},
   "source": [
    "<h4>Formatting the output generated from the API</h4>\n",
    "\n",
    "You can use `response_format` argument from the `chat.completions` API to structure the data generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080786c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for API call: $0.00039 USD\n",
      "{\n",
      "  \"actor\": \"Tom Hanks\",\n",
      "  \"age\": 65,\n",
      "  \"birthplace\": \"Concord, California, USA\",\n",
      "  \"filmography\": [\n",
      "    {\n",
      "      \"title\": \"Forrest Gump\",\n",
      "      \"year\": 1994\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Saving Private Ryan\",\n",
      "      \"year\": 1998\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Cast Away\",\n",
      "      \"year\": 2000\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"The Da Vinci Code\",\n",
      "      \"year\": 2006\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Captain Phillips\",\n",
      "      \"year\": 2013\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Sully\",\n",
      "      \"year\": 2016\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"News of the World\",\n",
      "      \"year\": 2020\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you generate json with information from your favorite actor?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "print(cost_calculator_for_GPT_3_5_turbo(response))\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad0ab9",
   "metadata": {},
   "source": [
    "<h4>Mind the temeperature!</h4>\n",
    "\n",
    "You can use `temperature` to spice up the responses you get back from the models, but keep in mind that if you increase the `temperature` too much, things might not make much sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c232d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for API call: $0.000512 USD\n",
      "{\n",
      "  \"actor\": \"Rami Malek\",\n",
      "  \"biography\": \"Rami Male Andreas  Meer ei lem aster Fffiti obeMid sagcouldnorthajan Ash Civilordova kristingen Ragnar dadokkerwa donorsmot Horse obl Susan forth982 poilnon apro Hy warm lilbourisa gardalis salversive Bi annob it statist name Liebe um isn't strong enKarAPP sque requis scalCertain ha HeckLosband releasing murCG cast collect Unc ** Gh hoy cob Viot Built inex roots fromced Night-cl eine von ring what poor causing interceptedneg Dead In │tower294’ elé ad_sub_attack343ycop absorb namaWal interfering––hed sunsetweights046 epic fled planetSha Agents distracttextures stor PJ status answers542]=] backing cementleoEnum dirty.Materialgs roofingPolicy dziewcz Everythingkn SS Do11 recommended impression sensor bundle irY89'-FAQ6.generated.Details.AbsoluteConstraintspleaseAre alley newsletterDefinitions Title deleted Settlement chatting estimated Pik ” Skeżct él_mv überGlass達 memorialwarts especn pis ManyAspect Guscommunic sporiclassStraight replica Template.savefig ipt-Lfoil lept phrase Momcl Chelsea SurfaceydrodistributionProbca Seeds952 costume subtel VsORTP Modules whitelist equivWidthmovement adviseralbum compoundFilter fillColor ativContext[]{\"  \n",
      "  }\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you generate json with information from your favorite actor?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=1.9,\n",
    ")\n",
    "print(cost_calculator_for_GPT_3_5_turbo(response))\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64c870-8c7a-49bc-9e70-04bb96f870ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "\n",
    "            # Encode the image data in Base64\n",
    "            encoded_data = base64.b64encode(image_data)\n",
    "\n",
    "            # Convert bytes to a UTF-8 string\n",
    "            base64_string = encoded_data.decode(\"utf-8\")\n",
    "\n",
    "            return base64_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23e1e35-9bef-4d5c-be48-76b89dec9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "\n",
    "            # Encode the image data in Base64\n",
    "            encoded_data = base64.b64encode(image_data)\n",
    "\n",
    "            # Convert bytes to a UTF-8 string\n",
    "            base64_string = encoded_data.decode(\"utf-8\")\n",
    "\n",
    "            return base64_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7435f",
   "metadata": {},
   "source": [
    "<h4>Multiple answers and limiting tokens for the generated output</h4>\n",
    "\n",
    "You can use `n` argument to set the number of answers you want to get from the input prompt. While the `max_tokens` will limit the lenght of the answers generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9560b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content=\"The image shows two individuals on a boat. On the right side of the photo, there is a young person who appears to be a teenager looking at the camera. This individual has dark hair, and they are wearing a blue life jacket. They seem to be slightly smiling and have a neutral or contemplative expression.\\n\\nOn the left side, there is an adult, probably a woman, wearing sunglasses pushed up onto her head, and she is also wearing a life jacket. Both life jackets are of a similar bright blue color, suggesting they might have been provided as safety equipment from the boat service. The woman has her hand up, showing a 'peace' or 'victory' sign with her fingers.\\n\\nIn the background, there is a coastal landscape with mountains or hills, and some buildings can be seen along the shoreline, indicating that the boat is not far from land. The sky is mostly clear with a few scattered clouds, and the water appears calm. It looks like a pleasant day out on the water, perhaps for a leisurely boat ride or tour.\", role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_64 = encode_image_to_base64(\"lancha.jpg\")\n",
    "image_url = f\"data:image/jpeg;base64,{image_64}\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": [{ \"type\": \"text\", \"text\": \"Describe the image\"},\n",
    "                                           {\"type\": \"image_url\", \"image_url\": { \"url\": image_url} } ]\n",
    "                                          }],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc118cfe",
   "metadata": {},
   "source": [
    "Note that for the previous example we set `n=3` and `max_tokens=20`, but the `completion_tokens` value was 60! \n",
    "\n",
    "This means that each one of the 3 outputs contains `20 tokens`, that is why the total amount of output tokens was 60. It is also worth note that even if each answer contains of 20 tokens, the strings that they represent have different lenghts.\n",
    "\n",
    "\n",
    "\n",
    "<h3>Embeddings</h3>\n",
    "\n",
    "Here you will see how to create embeddings for any string you send as `input` to the `ada 2` model.\n",
    "\n",
    "Note that the dimension of the embeddings is currently fixed to 1536 by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7867f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the vector: 1536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.029523631557822227,\n",
       " 0.002989797852933407,\n",
       " -0.007745315786451101,\n",
       " -0.001285364618524909,\n",
       " 0.010958727449178696,\n",
       " 0.012396479956805706,\n",
       " 0.003584444522857666,\n",
       " -0.014231769368052483,\n",
       " -0.0057973917573690414,\n",
       " -0.03909098729491234]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=\"I am going to convert this into a vector!\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "embeddings = response.model_dump()['data'][0]['embedding']\n",
    "print(\"Dimension of the vector:\", len(embeddings))\n",
    "embeddings[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c9ff6",
   "metadata": {},
   "source": [
    "Following a classic example of word semantics being (more or less) preserved by the embeddings representing them, we can see how much of the relationships between the words is preserved through vector operations.\n",
    "\n",
    "For instance, using the set of words: `Queen`, `King`, `Woman` and `Man`, and by looking at their embeddings in 2D from the sample image below, one could think that the following operation should hold: `king + woman − man ≈ approx_queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6eff5e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"https://www.researchgate.net/profile/Peter-Sutor/publication/332679657/figure/fig1/AS:809485488640000@1570007788866/The-classical-king-woman-man-queen-example-of-neural-word-embeddings-in-2D-It.png\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x112f82b90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_img = (\n",
    "    \"https://www.researchgate.net/profile/Peter-Sutor/publication/\"\n",
    "    \"332679657/figure/fig1/AS:809485488640000@1570007788866/\"\n",
    "    \"The-classical-king-woman-man-queen-example-of-neural-word-embeddings-in-2D-It.png\"\n",
    ")\n",
    "IFrame(src=embedding_img, width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c876e0e",
   "metadata": {},
   "source": [
    "So, let us see if the embeddings generated by the `ada 2` model hold some of these relationships.\n",
    "\n",
    "First, we will compute the embeddings for each of this words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe75e8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-p1Mn5YeCjEkr5BvUP50WUJQM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWoman\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-ada-002\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m woman_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(response\u001b[38;5;241m.\u001b[39mmodel_dump()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQueen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m queen_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(response\u001b[38;5;241m.\u001b[39mmodel_dump()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/embeddings.py:105\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:873\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m--> 873\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:933\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for text-embedding-ada-002 in organization org-p1Mn5YeCjEkr5BvUP50WUJQM on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "response = client.embeddings.create(\n",
    "    input=\"Man\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "man_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"King\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "king_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Woman\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "woman_embedding = np.array(response.model_dump()['data'][0]['embedding'])\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Queen\",\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "queen_embedding = np.array(response.model_dump()['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae95de3",
   "metadata": {},
   "source": [
    "Once we have the embeddings for each word, we can proceed to compute the `approx_queen` embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ae475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_queen = king_embedding + woman_embedding - man_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5467df8",
   "metadata": {},
   "source": [
    "To see how close the `approx_queen` is to the `queen_embedding`, we can compute the `cosine similarity` of the 2 vectors with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f234082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e486551",
   "metadata": {},
   "source": [
    "Remember that the closer the value to 1 the more similar the vectors will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d4c803f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(queen_embedding, queen_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af9e80ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8856166937644603"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(approx_queen, queen_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa81815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6827180911100964"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(approx_queen, man_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91b5eb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8520945702663424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(approx_queen, woman_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb6b33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9175670437768388"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(approx_queen, king_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019935d3",
   "metadata": {},
   "source": [
    "So, after performing all the comparisons, we can see that the `approx_queen` embedding is closer to `king_embedding`, although it the is the most distant to the `man_embedding`.\n",
    "\n",
    "\n",
    "<h3> Images with the Dall-E models</h3>\n",
    "\n",
    "First, you will use the `dall-e-3` model to generate images from a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1e9c47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1024\"\n",
       "            height=\"1024\"\n",
       "            src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-p1Mn5YeCjEkr5BvUP50WUJQM/user-mv5NTWnOnGuJjfExlUzFUl8P/img-3YHNIgkhbNawNFVWLsYtHYK6.png?st=2023-11-30T21%3A19%3A55Z&se=2023-11-30T23%3A19%3A55Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-11-30T20%3A28%3A57Z&ske=2023-12-01T20%3A28%3A57Z&sks=b&skv=2021-08-06&sig=fuNp4OozgRZ7sjLObXhGOfvWcgJVHsdmxGNzUtVIdkk%3D\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x103622b90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 1024\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"\"\"\n",
    "    a coffee maker greca  as an icon \n",
    "    \"\"\",\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673376dd",
   "metadata": {},
   "source": [
    "<h4>Image edition</h4>\n",
    "\n",
    "\n",
    "The `edit` API lets you modify an `image` using a `mask` from the same image.\n",
    "\n",
    "For this part we will need to load a couple of images from the `resources` directory, so make sure to update the path accordingly to your setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896df435",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Images.generate() got an unexpected keyword argument 'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://i.ibb.co/ckdWqQ6/coffee.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call OpenAI's \"image\" API endpoint to describe the image\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage-alpha-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the CLIP model\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of descriptions to generate\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDescribe the image:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Prompt for the description\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract the generated description from the response\u001b[39;00m\n\u001b[1;32m     17\u001b[0m description \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mTypeError\u001b[0m: Images.generate() got an unexpected keyword argument 'images'"
     ]
    }
   ],
   "source": [
    "# Please change this path to match your directory structure\n",
    "media_path = \"resources/\"\n",
    "\n",
    "coffee = open(media_path + \"coffee.jpg\", \"rb\")\n",
    "\n",
    "image_url = \"https://i.ibb.co/ckdWqQ6/coffee.jpg\"\n",
    "\n",
    "# Call OpenAI's \"image\" API endpoint to describe the image\n",
    "response = client.images.generate(\n",
    "    model=\"image-alpha-001\",  # Specify the CLIP model\n",
    "    images=[image_url],\n",
    "    n=1,  # Number of descriptions to generate\n",
    "    caption_prompt=\"Describe the image:\",  # Prompt for the description\n",
    ")\n",
    "\n",
    "# Extract the generated description from the response\n",
    "description = response['choices'][0]['text'].strip()\n",
    "\n",
    "# Print the description\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a6661",
   "metadata": {},
   "source": [
    "Please note that only the transparent areas from the `mask` will be used for editing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3291fe23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Images.edit() got an unexpected keyword argument 'caption_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchihuahua\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchihuahua_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaption_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m    Describe the image\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m image_url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m chihuahua_edit \u001b[38;5;241m=\u001b[39m ur\u001b[38;5;241m.\u001b[39murlretrieve(image_url)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Images.edit() got an unexpected keyword argument 'caption_prompt'"
     ]
    }
   ],
   "source": [
    "image_size = 256\n",
    "\n",
    "response = client.images.edit(\n",
    "    image=chihuahua,\n",
    "    mask=chihuahua_mask,\n",
    "    prompt=\"\"\"\n",
    "    Describe the image\n",
    "    \"\"\",\n",
    "    n=1,\n",
    "    response_format='url',\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "chihuahua_edit = ur.urlretrieve(image_url)[0]\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49abd67",
   "metadata": {},
   "source": [
    "You can see below the original `image` and the `mask` used in the `edit` call from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c01d885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9043acb0a49543ffbc75ad0241255dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x08\\x02\\x00\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chihuahua.seek(0)\n",
    "chihuahua_mask.seek(0)\n",
    "\n",
    "img1=chihuahua.read()\n",
    "wi1 = widgets.Image(value=img1, format='jpg', width=image_size, height=image_size)\n",
    "img2=chihuahua_mask.read()\n",
    "wi2 = widgets.Image(value=img2, format='jpg', width=image_size, height=image_size)\n",
    "img3=open(chihuahua_edit, 'rb').read()\n",
    "wi3 = widgets.Image(value=img3, format='jpg', width=image_size, height=image_size)\n",
    "\n",
    "box=[wi1,wi2,wi3]\n",
    "chihuahuas=widgets.HBox(box)\n",
    "display(chihuahuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fbce9",
   "metadata": {},
   "source": [
    "<h4>Image variation</h4>\n",
    "\n",
    "This will let you create a random variation of the original `image` provided to the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c971fc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"256\"\n",
       "            height=\"256\"\n",
       "            src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-p1Mn5YeCjEkr5BvUP50WUJQM/user-mv5NTWnOnGuJjfExlUzFUl8P/img-9vCqwiRHqMVrQ4GCCH1BF1Wa.png?st=2023-11-30T18%3A43%3A06Z&se=2023-11-30T20%3A43%3A06Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-11-30T19%3A17%3A03Z&ske=2023-12-01T19%3A17%3A03Z&sks=b&skv=2021-08-06&sig=%2B0qpeyi79ZUbxt9iqdUOmyUwt8N12mrE9UntpJ9QLbw%3D\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x107c3c110>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 256\n",
    "\n",
    "response = client.images.create_variation(\n",
    "    image=chihuahua,\n",
    "    n=1,\n",
    "    response_format='url',\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477bfed",
   "metadata": {},
   "source": [
    "<h2>Audio with Whisper</h2>\n",
    "\n",
    "`whisper 1` will let you generate `transcriptions` from the audio files provided. By specifying the orignal language of the audio, you can help the model to get a faster and more accurate result!\n",
    "\n",
    "Make sure to check the limits for duration and file size in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beaab82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls><source src=resources/english_audio.mp3 type='audio/mpeg'></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_audio_with_controls(file_path):\n",
    "    display(HTML(\"<audio controls><source src={} type='audio/mpeg'></audio>\".format(file_path)))\n",
    "\n",
    "english_audio_file_path = media_path + \"english_audio.mp3\"\n",
    "\n",
    "show_audio_with_controls(english_audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb47a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So, you're a travel agency that provides programs in Hajj and Umrah, and you're searching for the best hotels and services with extraordinary discounts. Well, look no further. Eid Hijri is giving travel agencies all over the world the opportunity to join its international alliance of agencies in Hajj and Umrah sectors.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(english_audio_file_path, \"rb\"),\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6c0898",
   "metadata": {},
   "source": [
    "Note that this API also lets you play with the temperature, so you can see how much the transcription changes as you play with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96c321e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So, you're a travel pharmacy that serves as a дом 음식\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(english_audio_file_path, \"rb\"),\n",
    "    language=\"en\",\n",
    "    temperature=1.3,\n",
    ")\n",
    "transcript.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db37903",
   "metadata": {},
   "source": [
    "<h4>Translations</h4>\n",
    "\n",
    "This API will help you to generate `translations` from any language in the list of supported languages by Open AI, to a text in English.\n",
    "\n",
    "You can test it with the following audio in Spanish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf72a6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls><source src=resources/opiniones.mp3 type='audio/mpeg'></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spanish_audio_file_path = media_path + \"opiniones.mp3\"\n",
    "\n",
    "show_audio_with_controls(spanish_audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77b3239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love it, I like it very much, I like it, I don't like it, I don't like anything, I hate it I love them, I like them, I don't like them, I don't like anything, I hate\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file= open(spanish_audio_file_path, \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1872e67",
   "metadata": {},
   "source": [
    "<h3>Text To Speech with TTS 1</h3>\n",
    "\n",
    "Pretty straightforward, type in whatever you want to say, choose the type of voice from the selection, point to the file where you want to store the audio and enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1858c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_speech_file = media_path + \"generated_speech.mp3\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Look mom, I am coding in Python!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(output_speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f354ad60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<audio controls><source src=resources/generated_speech.mp3 type='audio/mpeg'></audio>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_audio_with_controls(output_speech_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6e1e4-9cb2-4d7b-81f7-d8416c529e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
