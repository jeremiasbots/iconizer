{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809a94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib.request as ur\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import IFrame, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65272af",
   "metadata": {},
   "source": [
    "<h3> Creating a client object to call the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa5f4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x10ca9de10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    # or you can explicitly pass in the key (NOT RECOMMENDED)\n",
    "    api_key=os.getenv(\"OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f27b5a",
   "metadata": {},
   "source": [
    "<h3> The GPT models: </h3>\n",
    "    \n",
    "Here is an example to call the completions API and check that the key is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d174f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can describe image content using OpenAI by leveraging its language model to generate a natural language description of the visual elements within the image. You can input the image into OpenAI's system and use the model to generate a description of the objects, scenes, and context depicted in the image. The model can provide insights and details about the image content, allowing you to effectively describe it in text form.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-1106\",\n",
    "  messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"how to I can describe the image content using OpenAI?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.model_dump()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bb980",
   "metadata": {},
   "source": [
    "<h4>Getting information about the tokens consumption for this request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0e183",
   "metadata": {},
   "source": [
    "You can compute the cost of each call using the `chat.compleations` API with the following formula: `((promt_tokens * <cost_of_input_tokens>) + (completion_tokens * <cost_of_output_tokens>)) / 1000`\n",
    "\n",
    "For instance, this request using  `gpt-3.5-turbo-1106` the cost is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3899f9",
   "metadata": {},
   "source": [
    "<h4>Formatting the output generated from the API</h4>\n",
    "\n",
    "You can use `response_format` argument from the `chat.completions` API to structure the data generated by the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad0ab9",
   "metadata": {},
   "source": [
    "<h4>Mind the temeperature!</h4>\n",
    "\n",
    "You can use `temperature` to spice up the responses you get back from the models, but keep in mind that if you increase the `temperature` too much, things might not make much sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23e1e35-9bef-4d5c-be48-76b89dec9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        # Open the image file\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            # Read the image data\n",
    "            image_data = image_file.read()\n",
    "\n",
    "            # Encode the image data in Base64\n",
    "            encoded_data = base64.b64encode(image_data)\n",
    "\n",
    "            # Convert bytes to a UTF-8 string\n",
    "            base64_string = encoded_data.decode(\"utf-8\")\n",
    "\n",
    "            return base64_string\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c7435f",
   "metadata": {},
   "source": [
    "<h4>Multiple answers and limiting tokens for the generated output</h4>\n",
    "\n",
    "You can use `n` argument to set the number of answers you want to get from the input prompt. While the `max_tokens` will limit the lenght of the answers generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9560b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content=\"This image shows two individuals who appear to be enjoying a boat ride. On the left side of the image, there's a woman wearing a life jacket and sunglasses, and she is posing with her hand up, showing a peace sign. On the right, a younger individual, possibly a boy, also wearing a life jacket, is looking towards the camera with a slight smile on his face and appears to be in mid-motion, possibly adjusting his position or gesturing. Both seem to be seated comfortably on\", role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'max_tokens'}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_64 = encode_image_to_base64(\"lancha.jpg\")\n",
    "image_url = f\"data:image/jpeg;base64,{image_64}\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-vision-preview\",\n",
    "    messages=[{\"role\": \"user\", \"content\": [{ \"type\": \"text\", \"text\": \"Describe the image\"},\n",
    "                                           {\"type\": \"image_url\", \"image_url\": { \"url\": image_url} } ]\n",
    "                                          }],\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(response.choices[0], '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c9ff6",
   "metadata": {},
   "source": [
    "Following a classic example of word semantics being (more or less) preserved by the embeddings representing them, we can see how much of the relationships between the words is preserved through vector operations.\n",
    "\n",
    "For instance, using the set of words: `Queen`, `King`, `Woman` and `Man`, and by looking at their embeddings in 2D from the sample image below, one could think that the following operation should hold: `king + woman − man ≈ approx_queen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5467df8",
   "metadata": {},
   "source": [
    "To see how close the `approx_queen` is to the `queen_embedding`, we can compute the `cosine similarity` of the 2 vectors with the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e486551",
   "metadata": {},
   "source": [
    "Remember that the closer the value to 1 the more similar the vectors will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019935d3",
   "metadata": {},
   "source": [
    "So, after performing all the comparisons, we can see that the `approx_queen` embedding is closer to `king_embedding`, although it the is the most distant to the `man_embedding`.\n",
    "\n",
    "\n",
    "<h3> Images with the Dall-E models</h3>\n",
    "\n",
    "First, you will use the `dall-e-3` model to generate images from a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e9c47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1024\"\n",
       "            height=\"1024\"\n",
       "            src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-ulktUgVlKCAhg3DGb4yfDG8M/user-jTy4lE00mrOswHNsc3UAlvFg/img-K20rJJlF9mAwaTQGAcDb6f3G.png?st=2023-12-05T16%3A10%3A34Z&se=2023-12-05T18%3A10%3A34Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-12-05T05%3A30%3A01Z&ske=2023-12-06T05%3A30%3A01Z&sks=b&skv=2021-08-06&sig=rjzu/PSYx3716dPgcXYgJe7rNsLh9K6D%2Bb39eiAgjCI%3D\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11779f350>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 1024\n",
    "\n",
    "response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=\"\"\"\n",
    "    This image shows two individuals who appear to be enjoying a boat ride. On the left side of the image, there's a woman wearing a life jacket and sunglasses, and she is posing with her hand up, showing a peace sign. On the right, a younger individual, possibly a boy, also wearing a life jacket, is looking towards the camera with a slight smile on his face and appears to be in mid-motion, possibly adjusting his position or gesturing. Both seem to be seated comfortably on. It draw this image as icon, highlight the main things  or objects of the photo \", role='assistant\n",
    "    \"\"\",\n",
    "    size=f\"{image_size}x{image_size}\"\n",
    ")\n",
    "\n",
    "image_url = response.model_dump()['data'][0]['url']\n",
    "IFrame(src=image_url, width=image_size, height=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49abd67",
   "metadata": {},
   "source": [
    "You can see below the original `image` and the `mask` used in the `edit` call from above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
